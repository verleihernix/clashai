## Models

A list of supported models.
- "gpt-4o": The GPT-4 OpenAI model.
- "chatgpt-4o-latest": The latest version of the ChatGPT model.
- "gpt-4-turbo": The GPT-4 Turbo model.
- "gpt-4o-2024-08-06": The GPT-4 OpenAI model trained on data up to August 6, 2024.
- "gpt-4o-mini": The GPT-4 OpenAI model with fewer parameters.
- "gpt-4": The GPT-4 OpenAI model.
- "hermes-3-llama-3.1-405b": The Hermes-3 Llama-3.1 model with 405 billion parameters.
- "gpt-4o-2024-05-13": The GPT-4 OpenAI model trained on data up to May 13, 2024.
- "gpt-4o-mini-2024-07-18": The GPT-4 OpenAI model with fewer parameters trained on data up to July 18, 2024.
- "llama-3.1-405b-instruct": The Llama-3.1 model with 405 billion parameters and instruction-based training.
- "qwen-2-7b-instruct": The Qwen-2 model with 7 billion parameters and instruction-based training.
- "nous-capybara-7b": The Nous Capybara model with 7 billion parameters.
- "phi-3-medium-128k-instruct": The Phi-3 model with medium size and instruction-based training.
- "openchat-7b": The OpenChat model with 7 billion parameters.
- "llama-3.1-70b-instruct": The Llama-3.1 model with 70 billion parameters and instruction-based training.
- "toppy-m-7b": The Toppy-M model with 7 billion parameters.
- "gemma-7b-it": The Gemma model with 7 billion parameters and instruction-based training.
- "mythomist-7b": The Mythomist model with 7 billion parameters.
- "phi-3-mini-128k-instruct": The Phi-3 model with mini size and instruction-based training.
- "gemma-2-9b-it": The Gemma-2 model with 9 billion parameters and instruction-based training.
- "llama-3-8b-instruct": The Llama-3 model with 8 billion parameters and instruction-based training.
- "mixtral-8x22b-v0.1": The Mixtral model with 8x22 billion parameters and version 0.1.
- "mixtral-8x22b-instruct-v0.1": The Mixtral model with 8x22 billion parameters, instruction-based training, and version 0.1.
- "llama-3-70b-instruct": The Llama-3 model with 70 billion parameters and instruction-based training.
- "llama-2-70b-chat-hf": The Llama-2 model with 70 billion parameters for chat and high-frequency prompts.
- "llama-2-13b-chat-hf": The Llama-2 model with 13 billion parameters for chat and high-frequency prompts.
- "llama-2-7b-chat-hf": The Llama-2 model with 7 billion parameters for chat and high-frequency prompts.
- "zephyr-7b-beta": The Zephyr model with 7 billion parameters in beta.
- "llama-3.1-8b-instruct": The Llama-3.1 model with 8 billion parameters and instruction-based training.
- "mixtral-8x7b-instruct-v0.1": The Mixtral model with 8x7 billion parameters, instruction-based training, and version 0.1.

- Type: 'gpt\-4o' \| 'chatgpt\-4o\-latest' \| 'gpt\-4\-turbo' \| 'gpt\-4o\-2024\-08\-06' \| 'gpt\-4o\-mini' \| 'gpt\-4' \| 'hermes\-3\-llama\-3.1\-405b' \| 'gpt\-4o\-2024\-05\-13' \| 'gpt\-4o\-mini\-2024\-07\-18' \| 'llama\-3.1\-405b\-instruct' \| 'qwen\-2\-7b\-instruct' \| 'nous\-capybara\-7b' \| 'phi\-3\-medium\-128k\-instruct' \| 'openchat\-7b' \| 'llama\-3.1\-70b\-instruct' \| 'toppy\-m\-7b' \| 'gemma\-7b\-it' \| 'mythomist\-7b' \| 'phi\-3\-mini\-128k\-instruct' \| 'gemma\-2\-9b\-it' \| 'llama\-3\-8b\-instruct' \| 'mixtral\-8x22b\-v0.1' \| 'mixtral\-8x22b\-instruct\-v0.1' \| 'llama\-3\-70b\-instruct' \| 'llama\-2\-70b\-chat\-hf' \| 'llama\-2\-13b\-chat\-hf' \| 'llama\-2\-7b\-chat\-hf' \| 'zephyr\-7b\-beta' \| 'llama\-3.1\-8b\-instruct' \| 'mixtral\-8x7b\-instruct\-v0.1'

- [Source](https://github.com/verleihernix/clashai/blob/580221aa13f2b13b59c03a36a2ac2c0e7c4a03b8/src/index.ts#L174)